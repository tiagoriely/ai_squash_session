
--------------------------------------SPARSE ----------------------------------------------------------------------------------

1. SPARSE

    issues:
    - scores always negative (happens if a word appears in more than half of the documents in my corpus, IDF volaue becomes negative)
    - retriever cannot find any good match but I know there are many in the corpus (too many is probably the problem)
    - highly specialised corpus with a lot of similar words (e.g. boast happens 10k+ times)
    - your queries are short and your session documents are long


    Solution 1 (not making a huge difference)- tuning BM25 parameters
        â€¢ k1 (Term Frequency Saturation): Controls how much the score increases with each additional appearance of a query term
        in a document. A lower k1 value reduces the impact of very high-frequency terms. Given your corpus has terms
        repeated thousands of times,
        ***lowering k1 (e.g., to 1.0 - 1.2) is a good idea***
        â€¢ b (Document Length Normalization): Controls how much longer documents are penalized compared to shorter ones.
        A higher b value ***(closer to 1.0, e.g., 0.85)*** increases this penalty. Since your queries are short and your session
        documents are long, a stronger penalty for length could help.


        just helped a little bit

     Solution 2 (not great, minimal useful but not making a difference) - stop words
     - Using the "minimal" stop word list for the meta-index strategy (always good to have)

     - a bit less negative with standard words but hard to guess which one to add in the list
     - using common words in the list as stop words just led to most scores = 0 and sometimes some negative (could not tell which session was the best at all)


     Solution 3 (good, but not on its own) - sparsing metadata
     FIRST POSITIVE SCORES!!!
     however limited as it looks as standalone words so words like "counter drop" are not caught properly
     ðŸš€ Running pipeline for query: 'A need a 45min session to improve my counter drop'




    -------------------------------------
    Tokenized Query: ['need', '45min', 'session', 'improve', 'counter', 'drop']

    Max score found: 1.3731

    Min score found: -1.8160

    Number of docs with score > 0: 57

    Total documents: 500

    -------------------------------------

    [DEBUG] top76: id=session_077, score=1.3731

    [DEBUG] top165: id=session_166, score=1.3511

    [DEBUG] top237: id=session_238, score=1.3403

    [DEBUG] top463: id=session_464, score=1.2570

    [DEBUG] top347: id=session_348, score=0.4493

    [DEBUG] top64: id=session_065, score=0.4011

    [DEBUG] top319: id=session_320, score=0.3827

    [DEBUG] top186: id=session_187, score=0.3804

    [DEBUG] top208: id=session_209, score=0.3804

    [DEBUG] top230: id=session_231, score=0.3804
    --- âœ… Experiment Complete ---
    Top retrieved doc ID: session_077


    My Analysis:
    The only problem is that it's actually not retrieving the appropriate sessions:
    and I think this is where 2/3/n-gram is going to help.

    Here is my process of thing:

    1) the query: 'A need a 45min session to improve my counter drop'
    2) Tokenized Query: ['need', '45min', 'session', 'improve', 'counter', 'drop']
    3) the most important word here was "counter drop" but it its separated into "drop" and "counter"
    4) so the sparse retriever found sessions with drops, in this case the straight drop, but actually the straight drop is not a counter drop in this case, therefore this session is actually not finding a good session, it's practicing another "drop"
    5) however if "counter drop" was seen as one one then it would have found more appropriate sessions:

    like session80:

    Solution 4



    END SPARSE
----------------------------------------------RETRIEVERS EVALUATION------------------------------------------------------------------------

Retriever Optimisation and Evaluation

    1. Building useful queries for research
        -1) expectation: low score (no session, or low similarity sessions found)
            building a list of common keywords never used in corpuses and creating prompts from this list
        -2) expectation: high distinguishable scores found (good sessions found)
        building 5 perfect prompts picking randomly 5 sessions from each grammar
        -3) expectation: high scores but not distinguishable
            vague questions(s) "generate a squash session"
        -4) then trying to have simple to complex questions using low to high number of combinations of metadata fields


Balanced 500:
    ## High-Level Conclusion ðŸ’¡
    No single retriever is sufficient on its own. Your results clearly show that each retriever has distinct and complementary strengths and weaknesses. The semantic retriever is good for general understanding but is unreliable. The sparse and field retrievers are brilliant with specifics but struggle with vague queries. This provides a powerful justification for using a hybrid fusion strategy.

    ## Semantic Retriever (e5)
    This retriever is good at finding the general "topical neighborhood" of a query but is not precise or trustworthy.

    Strengths:
        It performs reasonably well on vague, under-specified queries where keyword-based methods would fail.

    Weaknesses:
        Not Robust:
            It fails catastrophically on irrelevant, out-of-distribution queries, returning high-confidence scores for things that should be rejected. This makes it prone to "hallucinating" relevance.

        Low Discriminatory Power:
            The scores it assigns to the top 10-30 documents are extremely close together. It cannot confidently distinguish the single best result from other good ones.

        Insensitive to Detail: Its performance does not improve when more specific metadata is added to a query.

    ## Sparse Retriever (BM25)
    This retriever is a surprisingly powerful and reliable "keyword specialist."
    Strengths:
        Excellent with Specifics:
            Its performance improves dramatically as queries become more detailed and keyword-rich. It thrives on specific constraints.
        Robust and Trustworthy:
            It correctly returns very low or zero scores for irrelevant queries, making it highly reliable at rejecting bad inputs.

    Weaknesses:
        It fails on vague queries that lack strong, unique keywords.

    ## Field Metadata Retriever
    This retriever is a high-precision tool for structured queries but can be easily misled.
    Strengths:
        Superb with Structured Queries:
            It is the best performer for complex queries that contain multiple, specific metadata constraints (like duration, level, and participants).
    Weaknesses:
        Brittle and Not Robust:
            Like the semantic retriever, it fails on irrelevant queries if the parser accidentally extracts a valid keyword from the wrong context (e.g., finding "drop" in "badminton drop shot").


        ## Establishing a Relevance Threshold
            By analyzing how each retriever scores irrelevant ("out-of-distribution") and low-information ("under-specified") queries, you can define a score below which the results should be considered untrustworthy.

        ### Field Retriever Analysis
            Your analysis here is spot on.

            Observation:
                Irrelevant, out-of-context queries still produce scores, but they tend to stay below a certain level (you noted ~4.0). Meanwhile, even the vaguest relevant query scores a 5.0.

            Conclusion:
                You can propose a threshold. For instance, any result from the field_retriever with a score below 3.0 could be flagged as a low-confidence match, likely stemming from an accidental keyword match in an otherwise irrelevant query.

            Implication:
                This is a powerful mechanism for reliability. If the top-scoring document from the field retriever is below this threshold, the system could decide not to use it as context, or even ask the user for clarification.

        ### Semantic Retriever Analysis
            This is an equally important finding that directly addresses the semantic retriever's main weakness.
            Observation:
                Its primary failure is returning high scores for everything. However, you've found that even for queries from a similar domain (other racket sports), there is a floor to the scores (you noted a minimum of 0.721).
            Conclusion:
                You can establish a baseline. Although the scores are clustered, anything below a certain threshold (e.g., 0.70) is almost certainly not a useful document.
            Implication:
                This threshold acts as a filter to combat the retriever's tendency to "hallucinate" relevance. It allows you to discard the lowest-scoring results, which are most likely to be noise, before they ever reach the generator.