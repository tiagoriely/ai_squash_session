# evaluation/utils/ragas.py

from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy
from datasets import Dataset
from typing import Dict


class RagasEvaluator:
    """
    A modular evaluator that uses the RAGAS framework to assess
    the faithfulness and relevance of a generated answer.
    """

    def __init__(self):
        self.metrics = [
            faithfulness,
            answer_relevancy,
        ]

    def evaluate(self, query: str, context: str, generated_plan: str) -> Dict[str, float]:
        """
        Evaluates a single generation against its query and context.

        Args:
            query (str): The original user query.
            context (str): The context retrieved from the RAG system.
            generated_plan (str): The final plan generated by the LLM.

        Returns:
            Dict[str, float]: A dictionary containing the scores for each metric.
        """
        # RAGAS expects a Hugging Face Dataset object
        dataset = Dataset.from_dict({
            'question': [query],
            'contexts': [[context]],  # Note: contexts must be a list of strings
            'answer': [generated_plan]
        })

        print("  -> Evaluating with RAGAS (faithfulness, answer_relevancy)...")
        # Run the evaluation
        result = evaluate(dataset, self.metrics) # adding is_async=False can help with debugging

        # Convert the RAGAS EvaluationResult object to a standard dictionary
        # .to_pandas() creates a DataFrame, .iloc[0] gets the first row,
        # and .to_dict() converts it to the format needed for eval
        return result.to_pandas().iloc[0].to_dict()