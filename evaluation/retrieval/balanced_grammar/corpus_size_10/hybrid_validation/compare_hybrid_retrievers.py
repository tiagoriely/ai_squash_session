# scripts/compare_hybrid_retrievers.py

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from pathlib import Path

# PROJECT_ROOT = Path(__file__).resolve().parents[1]


def analyse_and_plot_hybrid_performance():
    """
    Loads, analyses, and visualises the performance comparison between
    the different hybrid retrieval strategies.
    """
    try:
        # Load the CSV file generated by the hybrid analysis script
        df = pd.read_csv("hybrid_retrievers_metrics.csv")
    except FileNotFoundError as e:
        print(f"Error: {e}. Please run 'analyse_hybrid_retrievers.py' first to generate the input CSV 'hybrid_retrievers_metrics.csv'")
        return

    # --- 1. Quantitative Analysis ---
    # Calculate the overall average metrics for each strategy
    overall_metrics = df.groupby('strategy_name').agg(
        avg_max_score=('max_score', 'mean'),
        avg_top_1_delta=('top_1_delta', 'mean'),
        avg_std_dev=('std_dev', 'mean')
    ).reset_index()

    # Define a logical order for the strategies in plots
    strategy_order = ['static_weighted_rrf', 'standard_unweighted_rrf', 'dynamic_query_aware_rrf']
    overall_metrics['strategy_name'] = pd.Categorical(overall_metrics['strategy_name'], categories=strategy_order,
                                                      ordered=True)
    overall_metrics = overall_metrics.sort_values('strategy_name')

    # Calculate the average max score per query type for each strategy
    category_metrics = df.groupby(['strategy_name', 'query_type'])['max_score'].mean().unstack()

    # --- 2. Print Analysis Summary to Console ---
    print("=" * 80)
    print("Final Empirical Analysis: Hybrid Retriever Strategy Comparison")
    print("=" * 80)
    print("\n### Overall Performance Metrics by Strategy:\n")
    print(overall_metrics.round(4).to_string(index=False))
    print("\n" + "=" * 80 + "\n")
    print("### Average Max Score by Query Category:\n")
    print(category_metrics.round(4))
    print("\n" + "=" * 80)

    # --- 3. Visualization ---
    sns.set_style("whitegrid")
    plt.rcParams['figure.figsize'] = (20, 7)
    plt.rcParams['font.size'] = 12
    fig = plt.figure()
    fig.suptitle('Comparison of Hybrid Retrieval Strategies', fontsize=20, y=1.03)

    # Plot 1: Overall Performance Comparison (Confidence and Relevance)
    ax1 = fig.add_subplot(1, 3, 1)
    overall_metrics.plot(x='strategy_name', y=['avg_max_score', 'avg_top_1_delta'], kind='bar', ax=ax1)
    ax1.set_title('Overall Performance (Relevance & Confidence)')
    ax1.set_xlabel('Hybrid Strategy')
    ax1.set_ylabel('Average Score')
    ax1.legend(['Avg. Max Score', 'Avg. Top-1 Delta'])
    plt.setp(ax1.get_xticklabels(), rotation=45, ha="right")

    # Plot 2: Performance on High-Relevance vs. Vague Queries
    ax2 = fig.add_subplot(1, 3, 2)
    # Filter for only the most interesting query types for this plot
    plot_categories = category_metrics.filter(regex="High-Relevance|Vague").T
    plot_categories.plot(kind='bar', ax=ax2, width=0.8)
    ax2.set_title('Performance on Key Query Types')
    ax2.set_xlabel('Query Category')
    ax2.set_ylabel('Average Max Score')
    ax2.legend(title='Hybrid Strategy')
    plt.setp(ax2.get_xticklabels(), rotation=45, ha="right")

    # Plot 3: Distribution of Max Scores for Each Strategy
    ax3 = fig.add_subplot(1, 3, 3)
    sns.boxplot(x='strategy_name', y='max_score', data=df, ax=ax3, hue='strategy_name', palette='viridis', legend=False)
    ax3.set_title('Overall Score Distribution by Strategy')
    ax3.set_xlabel('Hybrid Strategy')
    ax3.set_ylabel('Max Fusion Score')
    plt.setp(ax3.get_xticklabels(), rotation=45, ha="right")

    plt.tight_layout(rect=[0, 0, 1, 0.97])

    # --- 4. Save the Output ---
    output_filename = "hybrid_retriever_comparison.png"
    plt.savefig(output_filename, bbox_inches='tight')
    print(f"\nGenerated final comparison plots: {output_filename}")


if __name__ == "__main__":
    analyse_and_plot_hybrid_performance()