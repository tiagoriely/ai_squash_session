# Finetuning / LoRAÂ workflow for *ai\_squash\_session*

> **TL;DR**Â â€”â€¯Run `python finetuning/scripts/run_lora.py --config finetuning/configs/lora_qlora.yaml` to train a TinyLlamaâ€‘1.1â€¯B LoRA adapter on your squashâ€‘session JSON pairs, then `python finetuning/run_lora_generation.py` (or the snippet below) to generate new drills.

---

## 1.Â Directory layout

```
finetuning/
â”œâ”€ configs/            # YAML configs (training & inference)
â”‚  â””â”€ lora_qlora.yaml  # â† master config used by run_lora.py
â”œâ”€ data/
â”‚  â”œâ”€ finetune_splits/
â”‚  â”‚  â”œâ”€ train_instruct.jsonl    #  your "prompt â†” completion" pairs
â”‚  â”‚  â”œâ”€ valid_instruct.jsonl    #  (currently empty â€“ optional)
â”‚  â”‚  â””â”€ test_instruct.jsonl     #  heldâ€‘out set
â”‚  â””â”€ template_finetuning.json   #  JSON schema reference
â”œâ”€ checkpoints/         # saved LoRA adapters
â”‚  â””â”€ lora_crosslob/     # 1st run output (adapter_*.safetensors, etc.)
â”œâ”€ scripts/              # data prep + training utilities
â”‚  â”œâ”€ parse_docx_to_json.py   # DOCX â†’ JSON extractor
â”‚  â”œâ”€ make_pairs.py          # builds prompt/completion pairs
â”‚  â”œâ”€ split_pairs.py         # train/valid/test splitter
â”‚  â””â”€ run_lora.py            # ***main training entryâ€‘point***
â””â”€ run_lora_generation.py    # minimal generation helper
```

---

## 2.Â Preparing data

```bash
# 1) Extract sessions from Word docs â†’ raw JSON records
python finetuning/scripts/parse_docx_to_json.py

# 2) Wrap each record into a prompt/completion pair
python finetuning/scripts/make_pairs.py \
       --in  finetuning/data/train.jsonl \
       --out finetuning/data/finetune_splits/pairs.jsonl

# 3) Split pairs into train / valid / test (80â€‘10â€‘10)
python finetuning/scripts/split_pairs.py
```

*All three steps are idempotent â€“ feel free to rerun after adding more DOCX files.*

---

## 3.Â Training

*Config:* `finetuning/configs/lora_qlora.yaml`

Key knobs already tuned for an Appleâ€‘Silicon MacBook:

| parameter                     | value                    | note                                    |
| ----------------------------- | ------------------------ | --------------------------------------- |
| `model_name_or_path`          | TinyLlamaâ€‘1.1Bâ€‘Chatâ€‘v1.0 | fits in 9â€¯GB MPS pool                   |
| `per_device_train_batch_size` | **1**                    | smallest batch to avoid OOM             |
| `gradient_accumulation_steps` | 4                        | keeps effective batch = 4               |
| `fp16 / bf16`                 | `false`                  | CPU/MPS canâ€™t do fp16                   |
| `num_train_epochs`            | 3 (demo)                 | bump to 6â€“10 once you have â‰¥â€¯50 samples |

```bash
python finetuning/scripts/run_lora.py \
       --config finetuning/configs/lora_qlora.yaml
```

*Runtime* on Mâ€‘series â‰ˆâ€¯1â€¯h per 1â€¯000 steps; console shows a tqdm bar.

Training artefacts land in `finetuning/checkpoints/<runâ€‘name>`.

---

## 4.Â Generation

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel, PeftConfig

CKPT = "finetuning/checkpoints/lora_crosslob"

cfg   = PeftConfig.from_pretrained(CKPT)
base  = AutoModelForCausalLM.from_pretrained(cfg.base_model_name_or_path,
                                             torch_dtype="auto")
model = PeftModel.from_pretrained(base, CKPT)
model.eval()

 tok = AutoTokenizer.from_pretrained(cfg.base_model_name_or_path, use_fast=True)
 tok.pad_token = tok.eos_token

prompt = (
    "Design a 45â€‘minute squash drill session for an advanced player that focuses on: lobs."\
    "\n\nReturn **only** JSON in the exact format provided during fineâ€‘tuning."
)
inputs  = tok(prompt, return_tensors="pt")
out_ids = model.generate(**inputs, max_new_tokens=400)
print(tok.decode(out_ids[0], skip_special_tokens=True))
```

Tip: add a *blank schema* to the prompt or use RAG to inject one of your gold JSON examples for more reliable structure.

---

## 5.Â Evaluation

```
python evaluation/eval_ragas.py \
       --pred finetuning/checkpoints/lora_crosslob/preds.jsonl \
       --gold finetuning/data/finetune_splits/test_instruct.jsonl
```

---

## 6.Â Troubleshooting cheatsheet

| symptom                                   | likely cause               | quick fix                                  |
| ----------------------------------------- | -------------------------- | ------------------------------------------ |
| **`MPS backend out of memory`**           | batch or sequence too big  | `batch_size=1`, `max_length=256`           |
| `unexpected keyword... TrainingArguments` | transformers version < arg | delete that field or update ğŸ¤— libs        |
| Output not valid JSON                     | too few training pairs     | grow dataset to 70â€“100 or use RAG template |

---

## 7.Â Next steps

* **Add more DOCX drills** â†’ rerun data pipeline â†’ train 5â€“10 epochs.
* Attach the LoRA to the retrievalâ€‘augmented pipeline in `rag/pipelines/`.
* Push adapter to HuggingFace Hub for easy sharing.

Questions / bugs? Open an issue or ping `@yourâ€‘name`. Happy drilling! ğŸ¾
