# ai\_squash\_session

A minimal sandbox that embeds **FlashRAG** (vendored in `third_party/flashrag`) so you can *own the code*, hack it, and use it to build a Retrievalâ€‘Augmented Generation (RAG) stack on top of your private documents.

---

## ğŸ“‚ Directory layout (after first run)

```text
ai_squash_session/
â”œâ”€â”€ data/                  # raw + processed corpora
â”‚   â”œâ”€â”€ raw/               #  â¤· drop .docx / .csv / .xlsx / .json here
â”‚   â””â”€â”€ my_kb.jsonl        #  â¤· generated JSONL corpus
â”œâ”€â”€ indexes/               # FAISS / BM25 indexes land here
â”‚   â””â”€â”€ my_kb/
â”‚       â””â”€â”€ e5-base-v2_Flat.index
â”œâ”€â”€ scripts/               # oneâ€‘off utilities
â”‚   â”œâ”€â”€ prepare_corpus.py  # converts raw â‡’ JSONL
â”‚   â””â”€â”€ check_index.py     # tiny index inspector
â”œâ”€â”€ src/                   # demo code you actually import/run
â”‚   â””â”€â”€ test_retrieve.py   # smoke test for retrieval
â”œâ”€â”€ third_party/flashrag/  # full FlashRAG source, editable & versioned
â””â”€â”€ README.md              # you are here
```

*(Everything elseâ€”virtualâ€‘env, PyCharm settings, etc.â€”stays outside the repo.)*

---

## ğŸš€ Quickâ€‘start: retrievalâ€‘only pathway

### 1Â Clone & pull FlashRAG as submodule

```bash
# oneâ€‘time setup
git clone https://github.com/<you>/ai_squash_session.git && cd ai_squash_session

git submodule update --init --depth 1   # pulls third_party/flashrag
```

### 2Â Create a virtualâ€‘env & install

```bash
python -m venv .venv && source .venv/bin/activate

pip install -e ./third_party/flashrag        # editable install of your FlashRAG copy
pip install faiss-cpu python-docx pandas openpyxl langid tqdm transformers
```

> **Appleâ€‘silicon users:** no CUDA available; the code already detects `mps` (Metal) or falls back to CPU.
> **Linux/NVIDIA:** install `faiss-gpu` & the matching CUDA PyTorch wheel if you want GPU search.

### 3Â Add files to `data/raw/`

Copy any `.docx`, `.csv`, `.xlsx`, or `.json` you care about into `data/raw/`.

### 4Â Generate the corpus JSONL

```bash
python scripts/prepare_corpus.py                # writes data/my_kb.jsonl
```

### 5Â Build a dense index (E5â€‘base)

```bash
python -m flashrag.retriever.index_builder \
  --retrieval_method e5-base-v2 \
  --model_path intfloat/e5-base-v2 \
  --corpus_path data/my_kb.jsonl \
  --save_dir indexes/my_kb \
  --faiss_type Flat
```

*Patches already applied in `third_party/flashrag` ensure this runs on CPU/MPS.*

### 6Â Smokeâ€‘test retrieval

```bash
python src/test_retrieve.py
```

Expected sample output:

```text
[0] Drill - Lob cross (repetitive).docx â†’ Duration: approx. 60â€‘70 min The sessionâ€¦
[1] practice_schedule.xlsx            â†’ ### Sheet1 Time  Coachâ€¦
```

### 7Â Inspect your index (optional)

```bash
python scripts/check_index.py
```

Shows vector count, dimension, and a quick nearestâ€‘neighbour sanity check.

---

## ğŸ”® Next steps

### Generation (coming soon)

*Placeholder.*
Youâ€™ll plug `DenseRetriever` (or `MultiRetrieverRouter`) into an LLM prompt builder, maybe via the `flashrag.generator` pipeline.
Add your instructions here once you stabilise on a generator setup.

### Reranking

* Enable `use_reranker` in the config and add a crossâ€‘encoder (e.g. `cross-encoder/ms-marco-MiniLM-L-6-v2`).

### Multiâ€‘modal search

* Build two separate indexes with `--index_modal text` and `--index_modal image`; use `MultiModalRetriever`.

---

## âœï¸ Maintaining your vendored FlashRAG

* **Edit** files in `third_party/flashrag/â€¦`, commit inside the submodule, then:

  ```bash
  cd third_party/flashrag && git add -u && git commit -m "feat: â€¦"
  cd ../.. && git add third_party/flashrag && git commit -m "Bump FlashRAG submodule"
  ```
* **Update**\* to latest upstream:

  ```bash
  cd third_party/flashrag
  git fetch origin && git merge origin/main
  cd ../..
  git add third_party/flashrag && git commit -m "FlashRAG upstream sync"
  ```

Happy hacking! ğŸš€
